{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 1\n",
    "##### 1) How would you define Machine Learning?\n",
    "##### 2) What are the differences between Supervised and Unsupervised Learning? Specify example 3 algorithms for each of these.\n",
    "##### 3) What are the test and validation set, and why would you want to use them?\n",
    "##### 4) What are the main preprocessing steps? Explain them in detail. Why we need to prepare our data?\n",
    "##### 5) How you can explore countionus and discrete variables?\n",
    "##### 6) Analyse the plot given below. (What is the plot and variable type, check the distribution and make comment about how you can preproccess it.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 1 Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-) Machine learning is a method used to train a model that automatically learns and improves itself using the data we have and to make this model make forward-looking inferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-) In supervised learning we use labeled data, we have dependent variable(categorical or continious), there are two main algorithms for supervised learning Regression and Classification. We can show multiple linear regression and polynomial regression as examples for regression. Decision trees and logistic regression are examples of classification.  \n",
    "In unsupervised learning we don't have a dependent variable, we basically do clustering.K-means, hierarchical clustering, and principal component analysis are examples of unsupervised learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-) Validation set is the sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration. Test set is the sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.The validation set is used to evaluate a given model, but this is for frequent evaluation. We use this data to fine-tune the model hyperparameters.The Test dataset provides the gold standard used to evaluate the model. It is only used once a model is completely trained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-)Data preprocessing is crucial in any data mining process as they directly impact success rate of the project. This reduces complexity of the data under analysis as data in real world is unclean.Data is said to be unclean if it is missing attribute, attribute values, contain noise or outliers and duplicate or wrong data. Presence of any of these will degrade quality of the results.\n",
    "Steps in Data preprocessing:\n",
    "\n",
    "1. Data cleaning:\n",
    "Data cleaning, also called data cleansing or scrubbing.\n",
    "Fill in missing values, smooth noisy data, identify or remove the outliers, and resolve inconsistencies.\n",
    "Data cleaning is required because source systems contain “dirty data” that must be cleaned.\n",
    "Steps in Data cleaning:\n",
    "\n",
    "1.1 Parsing:\n",
    "Parsing locates and identifies individual data elements in the source files and then isolates these data elements in the target files.\n",
    "Example includes parsing the first, middle and the last name.\n",
    "\n",
    "1.2 Correcting:\n",
    "Correct parsed individual data components using sophisticated data algorithms and secondary data sources.\n",
    "Example includes replacing a vanity address and adding a zip code.\n",
    "\n",
    "1.3 Standardizing:\n",
    "Standardizing applies conversion routines to transform data into its preferred and consistent format using both standard and custom business rules.\n",
    "Examples include adding a pre name, replacing a nickname.\n",
    "\n",
    "1.4 Matching:\n",
    "Searching and matching records within and across the parsed, corrected and standardized data based on predefined business rules to eliminate duplications.\n",
    "Examples include identifying similar names and addresses.\n",
    "\n",
    "1.5 Consolidating:\n",
    "Analyzing and identifying relationships between matched records and consolidating/merging them into one representation.\n",
    "\n",
    "1.6 Data cleansing must deal with many types of possible errors:\n",
    "These include missing data and incorrect data at one source.\n",
    "\n",
    "1.7 Data Staging:\n",
    "Accumulates data from asynchronous sources.\n",
    "At a predefined cutoff time, data in the staging file is transformed and loaded to the warehouse.\n",
    "There is usually no end user access to the staging file.\n",
    "An operational data store may be used for data staging.\n",
    "\n",
    "2. Data Reduction:\n",
    "\n",
    "Obtains reduced representation in volume but produces the same or similar analytical results.\n",
    "Need for data reduction:\n",
    "\n",
    "Reducing the number of attributes\n",
    "Reducing the number of attribute values\n",
    "Reducing the number of tuples\n",
    "\n",
    "3.  Discretization and Concept Hierarchy Generation(or summarization):\n",
    "\n",
    "Discretization: Reduce the number of values for a given continuous attribute by divide the range of a continuous attribute into intervals.\n",
    "Interval labels can then be used to replace actual data values.\n",
    "Concept Hierarchies: Reduce the data by collecting and replacing low level concepts(such as numeric values for the attribute age)by higher level concepts(such as young, middle-aged or senior)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5-) \n",
    "Discrete variables represent counts (e.g. the number of objects in a collection).\n",
    "\n",
    "Continuous variables represent measurable amounts (e.g. water volume or weight)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6-) We can say it is a continuous variable because data is measurable. Graph is a histogram. There are outliers in the data we can drop them. The data distrubuted between 0 and 2.5 cm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
